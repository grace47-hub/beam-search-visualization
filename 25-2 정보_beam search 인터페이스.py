"""
LLM ë””ì½”ë”© ê²°ì • ì‹œê°í™” ì¸í„°í˜ì´ìŠ¤
Greedy Search vs Beam Search ê¸°ë°˜ ë‹¨ì–´ ì„ íƒ ê³¼ì •ì˜ ì‹¤ì‹œê°„ ì‹œê°í™”
"""

import math
import warnings
import platform
from dataclasses import dataclass
from typing import List, Dict, Tuple, Any, Optional

import numpy as np
import pandas as pd
import torch
import torch.nn.functional as F
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import streamlit as st

from transformers import AutoTokenizer, AutoModelForCausalLM

warnings.filterwarnings('ignore')

# ----------------------------
# í•œê¸€ í°íŠ¸ ì„¤ì • (ê¸€ì ê¹¨ì§ ë°©ì§€)
# ----------------------------
def setup_korean_font():
    """
    ì‹œìŠ¤í…œì— ì„¤ì¹˜ëœ í•œê¸€ í°íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ì°¾ì•„ matplotlibì— ì„¤ì •
    Windows, Mac, Linux ëª¨ë‘ ì§€ì›
    """
    system = platform.system()
    
    # ì‹œìŠ¤í…œë³„ ìš°ì„  í°íŠ¸ ëª©ë¡
    font_candidates = []
    
    if system == 'Windows':
        font_candidates = ['Malgun Gothic', 'NanumGothic', 'NanumBarunGothic', 'Gulim']
    elif system == 'Darwin':  # macOS
        font_candidates = ['AppleGothic', 'Apple SD Gothic Neo', 'NanumGothic']
    else:  # Linux
        font_candidates = ['NanumGothic', 'NanumBarunGothic', 'UnDotum', 'Noto Sans CJK KR']
    
    # ì„¤ì¹˜ëœ í°íŠ¸ ì¤‘ì—ì„œ ì°¾ê¸°
    available_fonts = [f.name for f in fm.fontManager.ttflist]
    
    selected_font = None
    for font in font_candidates:
        if font in available_fonts:
            selected_font = font
            break
    
    # í°íŠ¸ ì„¤ì •
    if selected_font:
        plt.rcParams['font.family'] = selected_font
    else:
        # í°íŠ¸ë¥¼ ëª» ì°¾ìœ¼ë©´ ê¸°ë³¸ ì„¤ì •
        plt.rcParams['font.family'] = 'DejaVu Sans'
    
    # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
    plt.rcParams['axes.unicode_minus'] = False
    
    return selected_font

# ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ì‹¤í–‰
KOREAN_FONT = setup_korean_font()

# ----------------------------
# Configuration
# ----------------------------
AVAILABLE_MODELS = {
    "distilgpt2": "DistilGPT2 (ë¹ ë¦„, ê°€ë²¼ì›€)",
    "gpt2": "GPT2 (ë” ë‚˜ì€ í’ˆì§ˆ, ëŠë¦¼)",
    "gpt2-medium": "GPT2-Medium (ê³ í’ˆì§ˆ, ë§¤ìš° ëŠë¦¼)"
}

# ----------------------------
# Utilities
# ----------------------------
def set_seed(seed: int = 42):
    """ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
    torch.manual_seed(seed)
    np.random.seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)


def safe_token_str(tokenizer, token_id: int) -> str:
    """í† í°ì„ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜"""
    try:
        s = tokenizer.decode([token_id])
        # ë³´ê¸° ì¢‹ê²Œ ê³µë°±/ê°œí–‰ ì •ë¦¬
        s = s.replace("\n", "\\n").replace("\r", "\\r").replace("\t", "\\t")
        return s if s.strip() else f"<token_{token_id}>"
    except Exception:
        return f"<token_{token_id}>"


def topk_from_logits(logits: torch.Tensor, k: int) -> Tuple[np.ndarray, np.ndarray]:
    """
    logitsì—ì„œ ìƒìœ„ kê°œ í† í° ì¶”ì¶œ
    
    Args:
        logits: [vocab_size] í¬ê¸°ì˜ í…ì„œ
        k: ì¶”ì¶œí•  í† í° ê°œìˆ˜
    
    Returns:
        (topk_ids, topk_probs): numpy ë°°ì—´
    """
    probs = F.softmax(logits, dim=-1)
    topk_probs, topk_ids = torch.topk(probs, k=min(k, probs.numel()))
    return topk_ids.detach().cpu().numpy(), topk_probs.detach().cpu().numpy()


@dataclass
class StepTokenInfo:
    """ê° ìƒì„± ë‹¨ê³„ì˜ í† í° ì •ë³´"""
    step: int
    chosen_token_id: int
    chosen_token_str: str
    chosen_prob: float
    top_ids: List[int]
    top_probs: List[float]
    cumulative_logprob: float


@dataclass
class BeamState:
    """Beam Search ìƒíƒœ"""
    token_ids: List[int]
    score: float  # ëˆ„ì  ë¡œê·¸í™•ë¥  í•©


@dataclass
class BeamStepInfo:
    """Beam Search ê° ë‹¨ê³„ ì •ë³´"""
    step: int
    kept_beams: List[BeamState]
    pruned_count: int
    explored_count: int


# ----------------------------
# Model Loading
# ----------------------------
@st.cache_resource(show_spinner=False)
def load_model(model_name: str):
    """ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë”© (ìºì‹±ë¨)"""
    try:
        with st.spinner(f"ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}..."):
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(model_name)
            
            # EOS í† í°ì´ ì—†ìœ¼ë©´ PAD í† í° ì‚¬ìš©
            if tokenizer.eos_token_id is None:
                tokenizer.eos_token_id = tokenizer.pad_token_id
            
            model.eval()
            return tokenizer, model
    except Exception as e:
        st.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        st.stop()


# ----------------------------
# Core Decoding Functions
# ----------------------------
@torch.no_grad()
def next_logits(model, input_ids: torch.Tensor) -> torch.Tensor:
    """
    ë‹¤ìŒ í† í°ì˜ logits ê³„ì‚°
    
    Args:
        model: ì–¸ì–´ ëª¨ë¸
        input_ids: [1, seq_len] ì…ë ¥ í† í° ì‹œí€€ìŠ¤
    
    Returns:
        [vocab_size] í¬ê¸°ì˜ logits
    """
    out = model(input_ids=input_ids)
    return out.logits[0, -1, :]


@torch.no_grad()
def greedy_decode(
    tokenizer, 
    model, 
    prompt_text: str, 
    max_new_tokens: int, 
    top_n: int,
    progress_callback=None
) -> Dict[str, Any]:
    """
    Greedy Search ë””ì½”ë”©
    
    ë§¤ ë‹¨ê³„ë§ˆë‹¤ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ í† í°ë§Œ ì„ íƒ
    """
    try:
        input_ids = tokenizer.encode(prompt_text, return_tensors="pt", add_special_tokens=True)
        generated = input_ids.clone()
        
        steps: List[StepTokenInfo] = []
        cumulative_logprob = 0.0
        
        for step in range(1, max_new_tokens + 1):
            if progress_callback:
                progress_callback(step, max_new_tokens)
            
            logits = next_logits(model, generated)
            probs = F.softmax(logits, dim=-1)
            
            chosen_id = int(torch.argmax(probs).item())
            chosen_prob = float(probs[chosen_id].item())
            cumulative_logprob += float(torch.log(probs[chosen_id] + 1e-12).item())
            
            top_ids, top_probs = topk_from_logits(logits, top_n)
            
            info = StepTokenInfo(
                step=step,
                chosen_token_id=chosen_id,
                chosen_token_str=safe_token_str(tokenizer, chosen_id),
                chosen_prob=chosen_prob,
                top_ids=top_ids.tolist(),
                top_probs=top_probs.tolist(),
                cumulative_logprob=cumulative_logprob,
            )
            steps.append(info)
            
            # ë‹¤ìŒ í† í° ì¶”ê°€
            next_id_tensor = torch.tensor([[chosen_id]])
            generated = torch.cat([generated, next_id_tensor], dim=1)
            
            # EOS í† í°ì´ë©´ ì¤‘ë‹¨
            if tokenizer.eos_token_id is not None and chosen_id == tokenizer.eos_token_id:
                break
        
        text_out = tokenizer.decode(generated[0], skip_special_tokens=True)
        
        return {
            "text": text_out,
            "steps": steps,
            "final_logprob": cumulative_logprob,
            "final_prob": float(math.exp(cumulative_logprob)),
        }
    
    except Exception as e:
        st.error(f"âŒ Greedy ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None


@torch.no_grad()
def beam_decode(
    tokenizer, 
    model, 
    prompt_text: str, 
    max_new_tokens: int, 
    beam_width: int, 
    top_n: int,
    progress_callback=None
) -> Dict[str, Any]:
    """
    Beam Search ë””ì½”ë”©
    
    ê° ë‹¨ê³„ë§ˆë‹¤ beam_widthê°œì˜ í›„ë³´ë¥¼ ìœ ì§€í•˜ë©° íƒìƒ‰
    
    ì£¼ì˜: ê³„ì‚° íš¨ìœ¨ì„ ìœ„í•´ ê° beamì—ì„œ ìƒìœ„ beam_widthê°œ í† í°ë§Œ í™•ì¥í•©ë‹ˆë‹¤.
    (í‘œì¤€ Beam SearchëŠ” ì „ì²´ vocab í™•ì¥ í›„ ìƒìœ„ beam_widthê°œ ìœ ì§€)
    """
    try:
        input_ids = tokenizer.encode(prompt_text, return_tensors="pt", add_special_tokens=True)
        base_ids = input_ids[0].tolist()
        
        # ì´ˆê¸° beam: ë¹ˆ ì‹œí€€ìŠ¤
        beams: List[BeamState] = [BeamState(token_ids=[], score=0.0)]
        beam_steps: List[BeamStepInfo] = []
        bestpath_steps: List[StepTokenInfo] = []
        
        total_pruned = 0
        total_explored = 0
        
        for step in range(1, max_new_tokens + 1):
            if progress_callback:
                progress_callback(step, max_new_tokens)
            
            all_candidates: List[BeamState] = []
            explored = 0
            
            # ê° beam í™•ì¥
            for b in beams:
                full_ids = base_ids + b.token_ids
                full_tensor = torch.tensor([full_ids], dtype=torch.long)
                logits = next_logits(model, full_tensor)
                
                probs = F.softmax(logits, dim=-1)
                
                # ê³„ì‚° íš¨ìœ¨ì„ ìœ„í•´ ìƒìœ„ beam_widthê°œë§Œ í™•ì¥
                # (ì´ëŠ” í‘œì¤€ êµ¬í˜„ê³¼ ì•½ê°„ ë‹¤ë¥´ì§€ë§Œ ì‹¤ìš©ì )
                expand_k = min(beam_width * 2, probs.numel())  # ì•½ê°„ ì—¬ìœ ìˆê²Œ
                top_ids, top_probs = topk_from_logits(logits, expand_k)
                explored += len(top_ids)
                
                for tid, tp in zip(top_ids, top_probs):
                    tid = int(tid)
                    tp = float(tp)
                    new_score = b.score + math.log(max(tp, 1e-12))
                    all_candidates.append(
                        BeamState(token_ids=b.token_ids + [tid], score=new_score)
                    )
            
            # ìƒìœ„ beam_widthê°œë§Œ ìœ ì§€
            all_candidates.sort(key=lambda x: x.score, reverse=True)
            kept = all_candidates[:beam_width]
            pruned = max(0, len(all_candidates) - len(kept))
            
            total_pruned += pruned
            total_explored += explored
            
            beam_steps.append(
                BeamStepInfo(
                    step=step,
                    kept_beams=kept,
                    pruned_count=pruned,
                    explored_count=explored,
                )
            )
            
            beams = kept
            
            # Best beamì˜ stepë³„ í™•ë¥  ë¶„í¬ ê¸°ë¡ (íˆíŠ¸ë§µìš©)
            best = beams[0]
            if len(best.token_ids) > 0:
                # ì´ë²ˆ step ì„ íƒ ì§ì „ ìƒíƒœ
                full_ids_best = base_ids + best.token_ids[:-1]
                full_tensor_best = torch.tensor([full_ids_best], dtype=torch.long)
                logits_best = next_logits(model, full_tensor_best)
                
                probs_best = F.softmax(logits_best, dim=-1)
                chosen_id = best.token_ids[-1]
                chosen_prob = float(probs_best[chosen_id].item())
                
                top_ids_vis, top_probs_vis = topk_from_logits(logits_best, top_n)
                bestpath_steps.append(
                    StepTokenInfo(
                        step=step,
                        chosen_token_id=int(chosen_id),
                        chosen_token_str=safe_token_str(tokenizer, int(chosen_id)),
                        chosen_prob=chosen_prob,
                        top_ids=top_ids_vis.tolist(),
                        top_probs=top_probs_vis.tolist(),
                        cumulative_logprob=best.score,
                    )
                )
            
            # EOS ì¡°ê¸° ì¢…ë£Œ
            if (tokenizer.eos_token_id is not None and 
                len(best.token_ids) > 0 and 
                best.token_ids[-1] == tokenizer.eos_token_id):
                break
        
        # ìµœì¢… ê²°ê³¼
        best = beams[0]
        full_ids = base_ids + best.token_ids
        text_out = tokenizer.decode(full_ids, skip_special_tokens=True)
        
        # ë‹¤ì–‘ì„±: ìµœì¢… beamë“¤ì˜ ê³ ìœ  ì¶œë ¥ ê°œìˆ˜
        final_texts = []
        for b in beams:
            ids = base_ids + b.token_ids
            final_texts.append(tokenizer.decode(ids, skip_special_tokens=True))
        diversity = len(set(final_texts))
        
        return {
            "text": text_out,
            "bestpath_steps": bestpath_steps,
            "beam_steps": beam_steps,
            "final_logprob": best.score,
            "final_prob": float(math.exp(best.score)),
            "diversity": diversity,
            "total_pruned": total_pruned,
            "total_explored": total_explored,
            "final_candidates": final_texts[:5],  # ìƒìœ„ 5ê°œë§Œ
        }
    
    except Exception as e:
        st.error(f"âŒ Beam ë””ì½”ë”© ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None


# ----------------------------
# Visualization Functions
# ----------------------------
def steps_to_table(steps: List[StepTokenInfo]) -> pd.DataFrame:
    """ë‹¨ê³„ë³„ ì •ë³´ë¥¼ í…Œì´ë¸”ë¡œ ë³€í™˜"""
    rows = []
    for s in steps:
        rows.append({
            "ë‹¨ê³„": s.step,
            "ì„ íƒëœ í† í°": s.chosen_token_str,
            "í™•ë¥ ": f"{s.chosen_prob:.4f}",
            "ëˆ„ì  ë¡œê·¸í™•ë¥ ": f"{s.cumulative_logprob:.3f}",
        })
    return pd.DataFrame(rows)


def build_heatmap_data(
    steps: List[StepTokenInfo], 
    tokenizer
) -> Tuple[np.ndarray, List[str], List[str], List[List[str]]]:
    """íˆíŠ¸ë§µ ë°ì´í„° êµ¬ì„±"""
    num_steps = len(steps)
    if num_steps == 0:
        return np.array([]), [], [], []
    
    top_n = len(steps[0].top_ids)
    
    mat = np.zeros((num_steps, top_n), dtype=float)
    y_labels = [f"Step {s.step}" for s in steps]
    x_labels = [f"Top {i+1}" for i in range(top_n)]
    
    token_texts = [
        [safe_token_str(tokenizer, tid) for tid in s.top_ids] 
        for s in steps
    ]
    
    for i, s in enumerate(steps):
        mat[i, :] = np.array(s.top_probs, dtype=float)
    
    return mat, y_labels, x_labels, token_texts


def plot_heatmap(
    mat: np.ndarray, 
    y_labels: List[str], 
    x_labels: List[str], 
    token_texts: List[List[str]], 
    title: str
):
    """í† í° í™•ë¥  íˆíŠ¸ë§µ ê·¸ë¦¬ê¸° (í•œê¸€ ì§€ì› ê°œì„ )"""
    if mat.size == 0:
        return None
    
    # ë™ì  í¬ê¸° ê³„ì‚° (step ìˆ˜ì— ë”°ë¼ ë†’ì´ ì¡°ì •)
    height = max(6, min(0.6 * len(y_labels), 20))
    width = max(10, min(len(x_labels) * 0.8, 16))
    
    fig, ax = plt.subplots(figsize=(width, height))
    im = ax.imshow(mat, aspect="auto", cmap="YlOrRd")
    
    # ì œëª© ì„¤ì •
    ax.set_title(title, fontsize=14, pad=20, fontweight='bold')
    
    # Yì¶• ë ˆì´ë¸” (Step)
    ax.set_yticks(range(len(y_labels)))
    ax.set_yticklabels(y_labels, fontsize=10)
    ax.set_ylabel('ìƒì„± ë‹¨ê³„', fontsize=11, fontweight='bold')
    
    # Xì¶• ë ˆì´ë¸” (Rank)
    ax.set_xticks(range(len(x_labels)))
    ax.set_xticklabels(x_labels, rotation=45, ha="right", fontsize=9)
    ax.set_xlabel('í† í° ìˆœìœ„', fontsize=11, fontweight='bold')
    
    # ì…€ì— í† í° ë¬¸ìì—´ í‘œì‹œ (step ìˆ˜ê°€ ë§ìœ¼ë©´ ì¼ë¶€ë§Œ)
    max_show_steps = min(25, len(y_labels))  # ìµœëŒ€ 25ê°œ stepë§Œ í…ìŠ¤íŠ¸ í‘œì‹œ
    
    for i in range(max_show_steps):
        for j in range(mat.shape[1]):
            tok = token_texts[i][j]
            
            # í† í° ë¬¸ìì—´ ê¸¸ì´ ì œí•œ
            if len(tok) > 10:
                tok = tok[:8] + "â€¦"
            
            # ë°°ê²½ìƒ‰ì— ë”°ë¼ í…ìŠ¤íŠ¸ ìƒ‰ìƒ ê²°ì •
            text_color = "white" if mat[i, j] > 0.5 else "black"
            
            # í™•ë¥ ê°’ í‘œì‹œ (ë†’ì€ í™•ë¥ ë§Œ)
            if mat[i, j] > 0.1:  # 10% ì´ìƒë§Œ í‘œì‹œ
                label = f"{tok}\n{mat[i,j]:.2f}"
            else:
                label = tok
            
            ax.text(
                j, i, label,
                ha="center", va="center",
                fontsize=8, color=text_color,
                weight="bold" if mat[i, j] > 0.3 else "normal"
            )
    
    # ì»¬ëŸ¬ë°” ì„¤ì •
    cbar = fig.colorbar(im, ax=ax, fraction=0.02, pad=0.04)
    cbar.set_label("í™•ë¥ ", rotation=270, labelpad=20, fontsize=10)
    cbar.ax.tick_params(labelsize=9)
    
    # ë ˆì´ì•„ì›ƒ ìµœì í™” (ë¹ˆì¹¸ ì œê±°)
    plt.tight_layout()
    
    return fig


def beam_to_simple_tree_table(
    tokenizer, 
    prompt: str, 
    beam_steps: List[BeamStepInfo], 
    max_show: int = 5
) -> pd.DataFrame:
    """
    Beam íŠ¸ë¦¬ë¥¼ ê°„ë‹¨í•œ í…Œì´ë¸”ë¡œ í‘œí˜„
    (Graphviz ëŒ€ì‹  ì‚¬ìš© ê°€ëŠ¥)
    """
    rows = []
    for bs in beam_steps:
        shown = bs.kept_beams[:max_show]
        for rank, b in enumerate(shown, 1):
            text = tokenizer.decode(
                tokenizer.encode(prompt, add_special_tokens=False) + b.token_ids,
                skip_special_tokens=True
            )
            rows.append({
                "ë‹¨ê³„": bs.step,
                "ìˆœìœ„": rank,
                "ì ìˆ˜": f"{b.score:.3f}",
                "ìƒì„± í…ìŠ¤íŠ¸": text[:100]
            })
    
    return pd.DataFrame(rows)


def try_graphviz_tree(
    tokenizer, 
    prompt: str, 
    beam_steps: List[BeamStepInfo], 
    max_show: int = 3
) -> Optional[str]:
    """
    Graphviz DOT ë¬¸ìì—´ ìƒì„±
    Graphvizê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì€ ê²½ìš° None ë°˜í™˜
    """
    try:
        import graphviz
        
        dot_lines = []
        dot_lines.append('digraph BeamTree {')
        dot_lines.append('rankdir=LR;')
        dot_lines.append('node [shape=box, fontsize=9, fontname="Arial"];')
        
        # Root ë…¸ë“œ
        prompt_short = prompt[:40].replace('"', '\\"')
        dot_lines.append(f'root [label="PROMPT\\n{prompt_short}..."];')
        
        prev_nodes = ["root"]
        
        for bs in beam_steps:
            curr_nodes = []
            shown = bs.kept_beams[:max_show]
            
            for i, b in enumerate(shown):
                node_name = f's{bs.step}_{i}'
                text = tokenizer.decode(
                    tokenizer.encode(prompt, add_special_tokens=False) + b.token_ids,
                    skip_special_tokens=True
                )
                text_short = text[:50].replace('"', '\\"').replace('\n', '\\n')
                label = f"Step {bs.step}\\nscore={b.score:.2f}\\n{text_short}"
                dot_lines.append(f'{node_name} [label="{label}"];')
                curr_nodes.append(node_name)
            
            # ê°„ë‹¨í•œ ì—°ê²° (ì‹¤ì œ parent ì¶”ì ì€ ë³µì¡í•˜ë¯€ë¡œ ìƒëµ)
            for pn in prev_nodes:
                for cn in curr_nodes:
                    dot_lines.append(f'{pn} -> {cn};')
            
            prev_nodes = curr_nodes
        
        dot_lines.append('}')
        return '\n'.join(dot_lines)
    
    except ImportError:
        return None


# ----------------------------
# Streamlit UI
# ----------------------------
def main():
    st.set_page_config(
        page_title="LLM ë””ì½”ë”© ì‹œê°í™”", 
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("LLM ë””ì½”ë”© ê²°ì • ì‹œê°í™”")
    st.markdown("**Greedy Search vs Beam Search** - ë‹¨ì–´ ì„ íƒ ê³¼ì •ì˜ ì‹¤ì‹œê°„ ì‹œê°í™”")
    
    # í°íŠ¸ ìƒíƒœ í‘œì‹œ (ì ‘ì„ ìˆ˜ ìˆê²Œ)
    with st.expander("ì‹œìŠ¤í…œ ì •ë³´", expanded=False):
        if KOREAN_FONT:
            st.success(f"í•œê¸€ í°íŠ¸: {KOREAN_FONT} (ê·¸ë˜í”„ í•œê¸€ í‘œì‹œ ê°€ëŠ¥)")
        else:
            st.warning("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ëŠ” ì˜ë¬¸ìœ¼ë¡œ í‘œì‹œë©ë‹ˆë‹¤.")
        st.info(f"ì‹œìŠ¤í…œ: {platform.system()}")
    
    # ì‚¬ì´ë“œë°” ì»¨íŠ¸ë¡¤
    with st.sidebar:
        st.header("ì„¤ì •")
        
        model_key = st.selectbox(
            "ëª¨ë¸ ì„ íƒ",
            options=list(AVAILABLE_MODELS.keys()),
            format_func=lambda x: AVAILABLE_MODELS[x],
            index=0
        )
        
        st.markdown("---")
        
        prompt = st.text_area(
            "í”„ë¡¬í”„íŠ¸ ì…ë ¥ (ì˜ì–´ ê¶Œì¥)",
            value="I love machine learning because",
            height=80,
            help="ìƒì„±ì„ ì‹œì‘í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”. GPT-2ëŠ” ì˜ì–´ì— ìµœì í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
        )
        
        if not prompt.strip():
            st.warning("í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”!")
        
        st.markdown("---")
        
        decoding = st.radio(
            "ë””ì½”ë”© ì•Œê³ ë¦¬ì¦˜",
            options=["Greedy", "Beam"],
            index=1,
            help="Greedy: ë§¤ ë‹¨ê³„ ìµœê³  í™•ë¥  í† í°ë§Œ ì„ íƒ\nBeam: ì—¬ëŸ¬ í›„ë³´ë¥¼ ìœ ì§€í•˜ë©° íƒìƒ‰"
        )
        
        beam_width = 1
        if decoding == "Beam":
            beam_width = st.slider(
                "Beam Width",
                min_value=1,
                max_value=10,
                value=5,
                step=1,
                help="ìœ ì§€í•  í›„ë³´ ê²½ë¡œ ìˆ˜"
            )
        
        st.markdown("---")
        
        max_new_tokens = st.slider(
            "ìƒì„±í•  í† í° ìˆ˜",
            min_value=5,
            max_value=40,
            value=20,
            step=5,
            help="í”„ë¡¬í”„íŠ¸ ì´í›„ ìƒì„±í•  í† í° ê°œìˆ˜"
        )
        
        top_n = st.slider(
            "íˆíŠ¸ë§µ í† í° ê°œìˆ˜",
            min_value=5,
            max_value=30,
            value=15,
            step=5,
            help="íˆíŠ¸ë§µì— í‘œì‹œí•  ìƒìœ„ í† í° ê°œìˆ˜"
        )
        
        st.markdown("---")
        
        col1, col2 = st.columns(2)
        with col1:
            run_btn = st.button("ì‹¤í–‰", use_container_width=True, type="primary")
        with col2:
            compare_btn = st.button("ë¹„êµ ì‹¤í—˜", use_container_width=True)
        
        st.markdown("---")
        st.caption("Beam Width = 1ì¼ ë•Œ Greedyì™€ ë™ì¼í•©ë‹ˆë‹¤")
    
    # ëª¨ë¸ ë¡œë”©
    if prompt.strip():
        tokenizer, model = load_model(model_key)
    else:
        st.info("ğŸ‘ˆ ì‚¬ì´ë“œë°”ì—ì„œ ì„¤ì •ì„ ì¡°ì •í•œ í›„ ì‹¤í–‰ ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")
        return
    
    # ë‹¨ì¼ ì‹¤í–‰
    if run_btn and prompt.strip():
        set_seed(42)
        
        # ì§„í–‰ í‘œì‹œ
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        def update_progress(step, total):
            progress = step / total
            progress_bar.progress(progress)
            status_text.text(f"ìƒì„± ì¤‘... {step}/{total} ë‹¨ê³„")
        
        if decoding == "Greedy":
            result = greedy_decode(
                tokenizer, model, prompt, max_new_tokens, top_n,
                progress_callback=update_progress
            )
        else:
            result = beam_decode(
                tokenizer, model, prompt, max_new_tokens, beam_width, top_n,
                progress_callback=update_progress
            )
        
        progress_bar.empty()
        status_text.empty()
        
        if result is None:
            st.error("ë””ì½”ë”© ì‹¤íŒ¨")
            return
        
        # ê²°ê³¼ í‘œì‹œ
        st.markdown("---")
        st.subheader("ìƒì„±ëœ í…ìŠ¤íŠ¸")
        st.info(result["text"])
        
        # Greedy ê²°ê³¼
        if decoding == "Greedy":
            st.markdown("---")
            st.subheader("ë‹¨ê³„ë³„ í† í° ì„ íƒ")
            df = steps_to_table(result["steps"])
            st.dataframe(df, use_container_width=True, hide_index=True)
            
            st.markdown("---")
            st.subheader("í† í° í™•ë¥  íˆíŠ¸ë§µ")
            st.caption("ê° ë‹¨ê³„ì—ì„œ ìƒìœ„ í™•ë¥ ì„ ê°€ì§„ í† í°ë“¤ì˜ ë¶„í¬")
            
            mat, ylab, xlab, tok_texts = build_heatmap_data(result["steps"], tokenizer)
            fig = plot_heatmap(
                mat, ylab, xlab, tok_texts,
                title="Greedy Search - ë‹¨ê³„ë³„ í† í° í™•ë¥  ë¶„í¬"
            )
            if fig:
                st.pyplot(fig)
            
            st.markdown("---")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("ìµœì¢… ë¡œê·¸í™•ë¥ ", f"{result['final_logprob']:.3f}")
            with col2:
                st.metric("ìµœì¢… í™•ë¥ ", f"{result['final_prob']:.6e}")
        
        # Beam ê²°ê³¼
        else:
            st.markdown("---")
            st.subheader("ìµœê³  ê²½ë¡œ ë‹¨ê³„ë³„ ì„ íƒ")
            st.caption("Beam Searchì—ì„œ ìµœì¢…ì ìœ¼ë¡œ ì„ íƒëœ ê²½ë¡œì˜ ë‹¨ê³„ë³„ í† í°")
            df = steps_to_table(result["bestpath_steps"])
            st.dataframe(df, use_container_width=True, hide_index=True)
            
            st.markdown("---")
            st.subheader("í† í° í™•ë¥  íˆíŠ¸ë§µ (ìµœê³  ê²½ë¡œ)")
            mat, ylab, xlab, tok_texts = build_heatmap_data(result["bestpath_steps"], tokenizer)
            fig = plot_heatmap(
                mat, ylab, xlab, tok_texts,
                title="Beam Search (ìµœê³  ê²½ë¡œ) - ë‹¨ê³„ë³„ í† í° í™•ë¥  ë¶„í¬"
            )
            if fig:
                st.pyplot(fig)
            
            st.markdown("---")
            st.subheader("Beam íƒìƒ‰ ìš”ì•½")
            
            summary_rows = []
            for bs in result["beam_steps"]:
                summary_rows.append({
                    "ë‹¨ê³„": bs.step,
                    "ìœ ì§€ëœ Beam": len(bs.kept_beams),
                    "íƒìƒ‰í•œ í›„ë³´": bs.explored_count,
                    "ì œê±°ëœ í›„ë³´": bs.pruned_count
                })
            st.dataframe(pd.DataFrame(summary_rows), use_container_width=True, hide_index=True)
            
            # Beam íŠ¸ë¦¬ ì‹œê°í™” ì‹œë„
            st.markdown("---")
            st.subheader("Beam Search íŠ¸ë¦¬")
            
            dot_str = try_graphviz_tree(tokenizer, prompt, result["beam_steps"], max_show=3)
            
            if dot_str:
                try:
                    st.graphviz_chart(dot_str)
                except Exception as e:
                    st.warning(f"Graphviz ë Œë”ë§ ì‹¤íŒ¨: {str(e)}")
                    st.info("ëŒ€ì‹  í…Œì´ë¸” í˜•íƒœë¡œ í‘œì‹œí•©ë‹ˆë‹¤.")
                    tree_df = beam_to_simple_tree_table(tokenizer, prompt, result["beam_steps"])
                    st.dataframe(tree_df, use_container_width=True, hide_index=True)
            else:
                st.info("Graphvizê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í…Œì´ë¸” í˜•íƒœë¡œ í‘œì‹œí•©ë‹ˆë‹¤.")
                tree_df = beam_to_simple_tree_table(tokenizer, prompt, result["beam_steps"])
                st.dataframe(tree_df, use_container_width=True, hide_index=True)
            
            st.markdown("---")
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ìµœì¢… ë¡œê·¸í™•ë¥ ", f"{result['final_logprob']:.3f}")
            with col2:
                st.metric("ë‹¤ì–‘ì„±", result['diversity'], help="ìµœì¢… í›„ë³´ ì¤‘ ê³ ìœ í•œ ì¶œë ¥ ê°œìˆ˜")
            with col3:
                st.metric("íƒìƒ‰ í›„ë³´ ìˆ˜", result['total_explored'])
            with col4:
                st.metric("ì œê±° í›„ë³´ ìˆ˜", result['total_pruned'])
            
            with st.expander("ìµœì¢… ìœ ì§€ëœ í›„ë³´ë“¤ (ìƒìœ„ 5ê°œ)"):
                for i, text in enumerate(result["final_candidates"], 1):
                    st.write(f"**{i}.** {text}")
    
    # ë¹„êµ ì‹¤í—˜
    if compare_btn and prompt.strip():
        st.markdown("---")
        st.header("ìë™ ë¹„êµ ì‹¤í—˜: Beam Width = 1, 3, 5")
        
        set_seed(42)
        widths = [1, 3, 5]
        results_data = []
        
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        for idx, w in enumerate(widths):
            status_text.text(f"Beam Width = {w} ì‹¤í–‰ ì¤‘... ({idx+1}/3)")
            
            result = beam_decode(tokenizer, model, prompt, max_new_tokens, w, top_n)
            
            if result:
                results_data.append({
                    "Beam Width": w,
                    "ìµœì¢… ë¡œê·¸í™•ë¥ ": f"{result['final_logprob']:.3f}",
                    "ë‹¤ì–‘ì„±": result['diversity'],
                    "íƒìƒ‰ í›„ë³´ ìˆ˜": result['total_explored'],
                    "ì œê±° í›„ë³´ ìˆ˜": result['total_pruned'],
                    "ìƒì„± í…ìŠ¤íŠ¸": result['text'][:100]
                })
            
            progress_bar.progress((idx + 1) / 3)
        
        progress_bar.empty()
        status_text.empty()
        
        if results_data:
            # í…Œì´ë¸”
            df_compare = pd.DataFrame(results_data)
            st.dataframe(df_compare, use_container_width=True, hide_index=True)
            
            # ê·¸ë˜í”„
            st.markdown("---")
            st.subheader("ë¹„êµ ê·¸ë˜í”„")
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                fig1, ax1 = plt.subplots(figsize=(6, 4.5))
                ax1.plot(
                    [r["Beam Width"] for r in results_data],
                    [r["íƒìƒ‰ í›„ë³´ ìˆ˜"] for r in results_data],
                    marker="o", linewidth=2.5, markersize=10,
                    color='#1f77b4', markerfacecolor='white', 
                    markeredgewidth=2
                )
                ax1.set_title("Beam Width vs íƒìƒ‰ í›„ë³´ ìˆ˜", fontsize=12, fontweight='bold', pad=15)
                ax1.set_xlabel("Beam Width", fontsize=11, fontweight='bold')
                ax1.set_ylabel("íƒìƒ‰ í›„ë³´ ìˆ˜", fontsize=11, fontweight='bold')
                ax1.grid(True, alpha=0.3, linestyle='--')
                ax1.tick_params(labelsize=10)
                # ê°’ ë ˆì´ë¸” ì¶”ê°€
                for i, r in enumerate(results_data):
                    ax1.annotate(
                        str(r["íƒìƒ‰ í›„ë³´ ìˆ˜"]), 
                        ([r["Beam Width"] for r in results_data][i], r["íƒìƒ‰ í›„ë³´ ìˆ˜"]),
                        textcoords="offset points", xytext=(0,10), ha='center',
                        fontsize=9, fontweight='bold'
                    )
                plt.tight_layout()
                st.pyplot(fig1)
            
            with col2:
                fig2, ax2 = plt.subplots(figsize=(6, 4.5))
                logprobs = [float(r["ìµœì¢… ë¡œê·¸í™•ë¥ "]) for r in results_data]
                ax2.plot(
                    [r["Beam Width"] for r in results_data],
                    logprobs,
                    marker="o", linewidth=2.5, markersize=10,
                    color='#2ca02c', markerfacecolor='white',
                    markeredgewidth=2
                )
                ax2.set_title("Beam Width vs ìµœì¢… ë¡œê·¸í™•ë¥ ", fontsize=12, fontweight='bold', pad=15)
                ax2.set_xlabel("Beam Width", fontsize=11, fontweight='bold')
                ax2.set_ylabel("ìµœì¢… ë¡œê·¸í™•ë¥ ", fontsize=11, fontweight='bold')
                ax2.grid(True, alpha=0.3, linestyle='--')
                ax2.tick_params(labelsize=10)
                # ê°’ ë ˆì´ë¸” ì¶”ê°€
                for i, r in enumerate(results_data):
                    ax2.annotate(
                        f"{logprobs[i]:.1f}", 
                        ([r["Beam Width"] for r in results_data][i], logprobs[i]),
                        textcoords="offset points", xytext=(0,10), ha='center',
                        fontsize=9, fontweight='bold'
                    )
                plt.tight_layout()
                st.pyplot(fig2)
            
            with col3:
                fig3, ax3 = plt.subplots(figsize=(6, 4.5))
                ax3.plot(
                    [r["Beam Width"] for r in results_data],
                    [r["ë‹¤ì–‘ì„±"] for r in results_data],
                    marker="o", linewidth=2.5, markersize=10,
                    color='#ff7f0e', markerfacecolor='white',
                    markeredgewidth=2
                )
                ax3.set_title("Beam Width vs ë‹¤ì–‘ì„±", fontsize=12, fontweight='bold', pad=15)
                ax3.set_xlabel("Beam Width", fontsize=11, fontweight='bold')
                ax3.set_ylabel("ë‹¤ì–‘ì„± (ê³ ìœ  ì¶œë ¥ ìˆ˜)", fontsize=11, fontweight='bold')
                ax3.grid(True, alpha=0.3, linestyle='--')
                ax3.tick_params(labelsize=10)
                # ê°’ ë ˆì´ë¸” ì¶”ê°€
                for i, r in enumerate(results_data):
                    ax3.annotate(
                        str(r["ë‹¤ì–‘ì„±"]), 
                        ([r["Beam Width"] for r in results_data][i], r["ë‹¤ì–‘ì„±"]),
                        textcoords="offset points", xytext=(0,10), ha='center',
                        fontsize=9, fontweight='bold'
                    )
                plt.tight_layout()
                st.pyplot(fig3)
            
            st.markdown("---")
            st.info("""
            **ê´€ì°° í¬ì¸íŠ¸**
            - Beam Widthê°€ ì¦ê°€í•˜ë©´ íƒìƒ‰ í›„ë³´ ìˆ˜(ê³„ì‚° ë¹„ìš©)ê°€ ì¦ê°€í•©ë‹ˆë‹¤.
            - í•˜ì§€ë§Œ ìµœì¢… ë¡œê·¸í™•ë¥ ê³¼ ë‹¤ì–‘ì„±ì€ íŠ¹ì • ì§€ì  ì´í›„ í¬ê²Œ ê°œì„ ë˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
            - ì´ëŠ” Beam Searchì˜ íš¨ê³¼ê°€ ìƒí™©ì— ë”°ë¼ ì œí•œì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.
            """)


if __name__ == "__main__":
    main()
